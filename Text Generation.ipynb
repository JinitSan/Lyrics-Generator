{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"Beatles.txt\",\"r\")\n",
    "lyrics = file.readlines()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(lyrics):\n",
    "    word_to_i = {}\n",
    "    i_to_word = {}\n",
    "    ind = 0\n",
    "    for i in lyrics:\n",
    "        for j in i.split():\n",
    "            if j in word_to_i:\n",
    "                continue\n",
    "            else:\n",
    "                word_to_i[j] = ind\n",
    "                i_to_word[ind] = j\n",
    "                ind+=1\n",
    "    word_to_i['<EOS>'] = ind\n",
    "    i_to_word[ind] = '<EOS>'\n",
    "    ind+=1\n",
    "    word_to_i['<PAD>'] = ind\n",
    "    i_to_word[ind] = '<PAD>'\n",
    "    return word_to_i,i_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_i,i_to_word = create_vocab(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vectors(lyrics,word_to_i):\n",
    "    X = []\n",
    "    Y = []\n",
    "    max_val = 15\n",
    "    for i in lyrics:\n",
    "        temp_x = []\n",
    "        temp_y = []\n",
    "        words = i.split()\n",
    "        if len(words)>max_val-1:\n",
    "            words = words[0:max_val-1]\n",
    "        for j in range(len(words)-1):\n",
    "            temp_x.append(word_to_i[words[j]])\n",
    "            temp_y.append(word_to_i[words[j+1]])\n",
    "        temp_x.append(word_to_i[words[len(words)-1]])\n",
    "        temp_x.append(word_to_i['<EOS>'])\n",
    "        temp_y.append(word_to_i['<EOS>'])\n",
    "        temp_y.append(word_to_i['<PAD>'])\n",
    "        pad = [word_to_i['<PAD>'] for _ in range(max_val-len(words)-1)]\n",
    "        temp_x+=pad\n",
    "        temp_y+=pad\n",
    "        X.append(temp_x)\n",
    "        Y.append(temp_y)\n",
    "    return X,Y\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = create_vectors(lyrics,word_to_i)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeatlesDataset(Dataset):\n",
    "    def __init__(self,X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "    def __getitem__(self,idx):\n",
    "        return (torch.tensor(X[idx]).to(torch.int64),torch.tensor(Y[idx]).to(torch.int64))\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BeatlesDataset(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(data,batch_size=8,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextGenerator,self).__init__()\n",
    "        self.lstm_size = 256\n",
    "        self.embedding_size = 300\n",
    "        self.num_layers = 3\n",
    "        vocab_len = 2240\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_len,embedding_dim=self.embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_size,hidden_size=self.lstm_size,num_layers=self.num_layers,dropout=0.2,batch_first=True)\n",
    "        self.fc = nn.Linear(self.lstm_size,vocab_len)\n",
    "        \n",
    "    def forward(self,X,prev_state):\n",
    "        embed = self.embedding(X)\n",
    "        output, state = self.lstm(embed,prev_state)\n",
    "        logits = self.fc(output)\n",
    "        return logits,state\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        return (torch.zeros(self.num_layers,batch_size,self.lstm_size),\n",
    "                torch.zeros(self.num_layers,batch_size,self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGenerator(dataloader,generator):\n",
    "    generator.train()\n",
    "    epochs = 30\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(generator.parameters(), lr=0.0005)\n",
    "    decayRate = 0.96\n",
    "    my_lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "    for epoch in range(epochs):\n",
    "        state_h, state_c = generator.init_state(8)\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred, (state_h, state_c) = generator(x, (state_h, state_c))\n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print({ 'epoch': epoch, 'loss': loss.item() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'loss': 3.5135185718536377}\n",
      "{'epoch': 1, 'loss': 2.240377902984619}\n",
      "{'epoch': 2, 'loss': 3.119553804397583}\n",
      "{'epoch': 3, 'loss': 2.7091763019561768}\n",
      "{'epoch': 4, 'loss': 3.153564929962158}\n",
      "{'epoch': 5, 'loss': 2.7334821224212646}\n",
      "{'epoch': 6, 'loss': 1.9841578006744385}\n",
      "{'epoch': 7, 'loss': 2.224437713623047}\n",
      "{'epoch': 8, 'loss': 1.826805591583252}\n",
      "{'epoch': 9, 'loss': 1.7699040174484253}\n",
      "{'epoch': 10, 'loss': 1.7331290245056152}\n",
      "{'epoch': 11, 'loss': 1.8287843465805054}\n",
      "{'epoch': 12, 'loss': 2.4480490684509277}\n",
      "{'epoch': 13, 'loss': 2.348189115524292}\n",
      "{'epoch': 14, 'loss': 1.7935538291931152}\n",
      "{'epoch': 15, 'loss': 1.3278963565826416}\n",
      "{'epoch': 16, 'loss': 2.018045425415039}\n",
      "{'epoch': 17, 'loss': 1.643039584159851}\n",
      "{'epoch': 18, 'loss': 1.7255892753601074}\n",
      "{'epoch': 19, 'loss': 0.9781566858291626}\n",
      "{'epoch': 20, 'loss': 1.4304991960525513}\n",
      "{'epoch': 21, 'loss': 1.2370593547821045}\n",
      "{'epoch': 22, 'loss': 1.198279619216919}\n",
      "{'epoch': 23, 'loss': 1.2211267948150635}\n",
      "{'epoch': 24, 'loss': 1.4082409143447876}\n",
      "{'epoch': 25, 'loss': 1.2114678621292114}\n",
      "{'epoch': 26, 'loss': 1.1684521436691284}\n",
      "{'epoch': 27, 'loss': 0.9399275183677673}\n",
      "{'epoch': 28, 'loss': 0.8289364576339722}\n",
      "{'epoch': 29, 'loss': 0.7966139316558838}\n"
     ]
    }
   ],
   "source": [
    "generator = TextGenerator()\n",
    "trainGenerator(dataloader,generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(i_to_word,word_to_i,generator,text):\n",
    "    generator.eval()\n",
    "    words = text.split()\n",
    "    state_h,state_c = generator.init_state(1)\n",
    "    for i in range(0,10):\n",
    "        x = torch.tensor([[word_to_i[w] for w in words]])\n",
    "        y_pred, (state_h, state_c) = generator(x, (state_h, state_c))\n",
    "                \n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        words.append(i_to_word[word_index])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def songmaker(i_to_word,word_to_i,generator,text):\n",
    "    initial = text\n",
    "    for i in range(15):\n",
    "        words = predict(i_to_word,word_to_i,generator,initial)\n",
    "        if '<EOS>' in words:\n",
    "            words = words[:words.index('<EOS>')]\n",
    "        print(' '.join(words))\n",
    "        initial = ' '.join(words[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
